---
title: "Top 20 most Harmful/Costly meteorological events in the US, years 1996-2011"
author: "Mathieu C."
date: "2 octobre 2019"
output:
  html_document:
    keep_md: yes
  pdf_document: default
subtitle: A NOAA database analysis for Health and Economic consequences
---

## Synopsis
The file "Storm Data" comes from the U.S. National Oceanic and Atmospheric  
Administration's (NOAA) and was accessible at [this url](https://d396qusza40orc.cloudfront.net/repdata%2Fdata%2FStormData.csv.bz2) on oct. 2nd 2019.  
  
The goal of the assessment is to produce a report that points at the most  
harmful and most costly meteorological events across the US for the years we  
have access to (1950 - 2011).  
  
More details can be found in the README.md file at [this GitHub url](https://github.com/Ravenpsycho/RepData_PeerAssessment2)

## Data Processing

### Data Loading  
We will begin by taking a look at the data:
```{r cache=TRUE}
# Downloading file (47Mb, might take a while depending on the connection and speeds)
# note that this step is cached and will be executed much faster if repeated:
myurl <- "https://d396qusza40orc.cloudfront.net/repdata%2Fdata%2FStormData.csv.bz2"
if (!file.exists("Storm_Data.csv.bz2")){
        download.file(myurl, destfile = "Storm_Data.csv.bz2")
}
# Reading in the dataframe, again, might take a while.
if (!exists("StormData")){
        StormData <- read.csv("Storm_Data.csv.bz2")
}
str(StormData)
```
  
### Check for NA's
  
From the above call to `str()` function, the database seems pretty organized.  
Let's check for missing values:
```{r}
na_check <- lapply(StormData, is.na)
na_check <- lapply(na_check, sum)
sort(unlist(na_check), decreasing = T)
```
  
We can see that the database is an "all or nothing" type: either a variable  
has (almost) all it's values filled in, or almost all the data is missing.  
  
Fortunately, in the "missing" camp, we have only two variables `F` and  
`COUNTYENDN`. Since these variables don't seem to be relevant to the questions  
at hands, we won't have to take further actions in their regard.  
  
### Data modifications

This will require some packages, let's load them:
```{r warning=FALSE}
library(dplyr)
library(ggplot2)
library(reshape2)
library(pdftools)
library(stringr)
library(stringdist)
```
  
Let's first create a second database, `StormData_mod`, for three main reasons:  
  
1. We avoid having to rebuild the original (and heavy) dataframe with every knit.  
2. We can keep track of changes if needed.  
3. We can subset the original dataframe to work faster.  

We will start by subsetting by date > 01.01.1996. Why? Because this is when  
people started tracking all 48 events, not just some of them (cf [this thread](https://www.coursera.org/learn/reproducible-research/discussions/weeks/4/threads/38y35MMiEeiERhLphT2-QA)).  

We'll keep only the variables we will later use as well.
  
```{r cache=TRUE}
testdate <- as.POSIXlt(
                as.character(StormData$BGN_DATE),
                format = "%m/%d/%Y %H:%M:%S") > "1996-01-01"
conditions <- (StormData$CROPDMG > 0 | StormData$PROPDMG > 0 |
                       StormData$INJURIES > 0 | StormData$FATALITIES > 0) & testdate 
StormData_mod <- subset(StormData, conditions)
StormData_mod <- StormData_mod[,c("EVTYPE","INJURIES", "FATALITIES", "CROPDMG", "CROPDMGEXP",
                              "PROPDMG", "PROPDMGEXP")]

```
  
After some exploration, we can notice something about `EVTYPE`. There are  
duplicates. For example: `THUNDERSTORM WIND` appears a whole lot of times under  
slightly different names.  
  
This is a problem. Events will need to be grouped under the official  
48 names below (found in the pdf file in code, from a link provided  
on Coursera):  
  
```{r}
pdf_url <- paste0("https://d396qusza40orc.cloudfront.net/",
                  "repdata%2Fpeer2_doc%2Fpd01016005curr.pdf")
my_pdf <- 
        paste0(pdf_text(pdf_url)[c(2:4)])
my_str <- unlist(strsplit(my_pdf, "(\\r)?\\n"))
my_str <- my_str[47:128]
my_str_short <- 
        str_match(string = my_str, 
                  pattern = "7.([0-9]+)[.]? +(.*?) *(?:[(][A-Z][)])? *[.]{3,}")
my_str_short <- toupper(my_str_short[!is.na(my_str_short[,3]),3])
rm(my_pdf, my_str)
my_str_long <- data.frame(Value = c(1:4, 
                                    rep(5,2), 6:12,
                                    rep(13,2), 14:16,
                                    rep(17,2), 18:24, 
                                    rep(25,2), 26:48), 
                          name = unlist(strsplit(my_str_short, "[/]")))
print(my_str_short)

```

After (*lots*) of trial and error, one of the main issues to match is the fact  
that some of the terms are abbreviated, like `TSTM` and `FLD`.  
  
We will try to fix them before the automated attempt at matchin them. 
  
While we're at it, there's a few strings that start with `NON` something  
as well as all the micellaneous names people came up with.

*NOTE:* This step has been done in a very (*very*) empiric way, if one section  
of the work needs to be improved in the future, it's this one.

```{r cache=TRUE}
patterns <- c("FLD", "NON-? ?\\w*", "TSTM", "(LIGHT)? *FREEZ.*RAIN",
              "URBAN/SML STREAM", "Ice jam flood [(]minor",
              "((HEAVY|HWY|HVY)? *PRECIP(ITATION)?|RAIN)", ".*HYPOTHERMIA.*",
              "TORRENTIAL RAINFALL",
              "HYPERTHERMIA", "AND LIGHTNING$", ".*GUSTY WIND/",
              "(HWY|HVY)", "(EXTREME|TO[R]*ENTIAL|DAMAGE|UNSEASON[A-Z]*)",
              "UNSEASONAB.* WARM", "UNSEASONAB.*COLD",
              "(C(OA)?ST(A)?L *FLOOD(ING)?|BEACH EROSION)", 
              "AGRICULTURAL", "TEMPERATURE", "(HEAVY SURF|.*)?/HIGH SURF",
              "(LIGHT|FALLING|LATE SEASON)? *SNOW(FALL|/ICE)? *(SQUALL)?",
              ".*MICROBURST.*", "(ADVISORY|HAZARDOUS|AND [A-Z]*$)",
              ".*HEAVY SNOW.*", "BLOWING DUST", "COASTAL FLOOD.*",
              "FOG")
replacements <-  c("FLOOD", "", "THUNDERSTORM", "FROST/FREEZE",
                   "", "FLOOD",
                   "HEAVY RAIN", "COLD",
                   "RAIN",
                   "HEAT", "", "",
                   "HEAVY", "", 
                   "HEAT", "COLD",
                   "COASTAL FLOOD",
                   "", "", "HIGH SURF",
                   "HEAVY SNOW",
                   "THUNDERSTORM WIND", "",
                   "HEAVY SNOW", "HIGH WIND", "COASTAL FLOOD",
                   "DENSE FOG")

StormData_mod$EVTYPE_corr <- as.character(StormData_mod$EVTYPE)
for (i in 1:length(patterns)){
        StormData_mod$EVTYPE_corr <- 
                gsub(patterns[i], replacements[i],
                     StormData_mod$EVTYPE_corr, ignore.case = T, perl = T)
}
StormData_mod$EVTYPE_corr <- factor(StormData_mod$EVTYPE_corr, 
                                    levels = unique(StormData_mod$EVTYPE_corr))
```

We'll then attempt to match the names obtained in `StormData_mod$EVTYPE_corr`:

```{r}
## The matching attempt:
matches <- 
        amatch(toupper(as.character(StormData_mod$EVTYPE_corr))
               , my_str_long$name, 
               method = "lcs", 
               maxDist = 10)

## Building a data frame of the gradual results, was useful for testing purposes
## and might be useful if improvement is needed.
df_test_amatch <- data.frame(EVTYPE = StormData_mod$EVTYPE,
                             corr = StormData_mod$EVTYPE_corr,
                             RESULT = my_str_short[my_str_long$Value[matches]])
StormData_mod$EVTYPE_corr <- factor(df_test_amatch$RESULT, 
                                    levels = unique(df_test_amatch$RESULT))
```


After all this wrangling we end up with a new and "clean" variable:

```{r}
summary(StormData_mod$EVTYPE_corr)
```



## Data Analysis
## Results
  
Now we have all the tools to create a new dataframe, with data grouped by `EVTYPE`  
and we'll take a look at the highest means of fatalities and injuries combined.
```{r}
health_by_event <- StormData_mod %>%
        group_by(EVTYPE_corr) %>%
        summarize(mean.fatalities = mean(FATALITIES, na.rm = T),
                  mean.injuries = mean(INJURIES, na.rm = T),
                  total.means = mean.fatalities + mean.injuries,
                  occurences = n()) %>%
        arrange(desc(total.means))
head(health_by_event, 10)
```
  
In our question we want to know how harmful an event is to population.  
My first thought was to sort the events by their total means (injuries  
and fatalities) combined as in the top 10 above.  
  
However, we first have to make sure that these events occur regularly to avoid  
putting too much weight into a deadly, harmful but also very unlikely event  
  
Let's see what the top most frequent events a.k.a. `occurences` looks like:
```{r}
head(health_by_event[order(health_by_event$occurences, decreasing = T),], 10)
```
This is interesting. We can see that some newcomers like `HAIL` or  
`THUNDERSTORM WIND` occur lots of times and they **are** harmful, not top  
10 harmful but still.  
  
Let's try creating an `index` variable. It will be the product of `occurences`  
by `total.means`.  

*Why?*  
  
Because this way, if an event is occuring a very low amount of time but has  
a very harmful outcome, it will be on par with a less harmful event that  
happens much more frequently. We will log(10) this index to avoid working  
with huge numbers.  
  
Let's do this and have a look:
```{r}
health_by_event <- transform(health_by_event, 
                         index = log((occurences * total.means), 10))
head(health_by_event[order(desc(health_by_event$index)),], 10)
```

Maybe a figure would be prettier:

```{r}
plotpop <- head(health_by_event[order(desc(health_by_event$index)),], 20)
plotpop$EVTYPE_corr <- factor(plotpop$EVTYPE_corr, 
                              levels = as.character(plotpop$EVTYPE_corr))

warncolors <- colorRampPalette(c("red", "gold3"))
g1 <- ggplot(plotpop[c(1:6)],
             aes(x=EVTYPE_corr, y = index, fill = EVTYPE_corr))
g1 + geom_bar(stat = "identity", position = "dodge") +
        ylab("Health issues index")+
        xlab(element_blank())+
        scale_fill_manual(values = warncolors(20))+
        theme_bw()+
        theme(legend.position = "none")+
        theme(axis.text.x = element_text(angle = 45, hjust = 0.95)) +
        labs(title = paste0("Top 20 events in decreasing order of Health issues",
                            " index (HII).\n HII = mean of injuries + mean ",
                            " of fatalities * total occurences (log10)\nData for ",
                            "years 1996 to 2011"))
        
```


Let's do the same with cost, using two new variables, `PROPDMG` and `CROPDMG`,  
holding numbers for *property damage* and *crop damage*, respectively, they  
first have to be ajusted with their exponent `EXP` counteparts:
```{r}
v_pattern <- c("[0-9]", "^$", "[+]", "[?-]", "[hH]", "[kK]", "[mM]", "[Bb]") 
v_repl <- c("10", "0", "1", "0", "100", "1000", "1000000", "1000000000")
## Modifying CROPDMGEXP
StormData_mod$CROPDMGEXP <- as.character(StormData_mod$CROPDMGEXP)
StormData_mod$PROPDMGEXP <- as.character(StormData_mod$PROPDMGEXP)

for (i in 1:8){
        StormData_mod$CROPDMGEXP <-
                gsub(v_pattern[i], v_repl[i], StormData_mod$CROPDMGEXP)
        StormData_mod$PROPDMGEXP <- 
                gsub(v_pattern[i], v_repl[i], StormData_mod$PROPDMGEXP)
} 
StormData_mod$CROPDMGEXP <- as.numeric(StormData_mod$CROPDMGEXP)
StormData_mod$PROPDMGEXP <- as.numeric(StormData_mod$PROPDMGEXP)
```




```{r}
cost_by_event <- StormData_mod %>%
        group_by(EVTYPE_corr) %>%
        summarise(mean.prop = mean(PROPDMG * PROPDMGEXP, na.rm = T),
                  mean.crop = mean(CROPDMG * CROPDMGEXP, na.rm = T),
                  mean.combi = mean.crop + mean.prop,
                  occurences = n(),
                  index = log(mean.combi*occurences, 10)) %>%
        arrange(desc(index))
head(cost_by_event[order(desc(cost_by_event$mean.combi)),])
```
If we arrange the data by `mean.combi` (as seen above), we can see that the  
most costly events have a very low occurence. Since the goal of this analysis  
is to produce a report supposedly for preparation towards events, the formerly  
used method of creating an index to weight events by `occurences` seems about  
right.
```{r}
head(cost_by_event, 20)
```

```{r}
plotpop2 <- head(cost_by_event[order(desc(cost_by_event$index)),], 20)
plotpop2$EVTYPE_corr <- factor(plotpop2$EVTYPE_corr, 
                               levels = as.character(plotpop2$EVTYPE_corr))

prettycolors2 <- colorRampPalette(c("darkgreen", "springgreen"))
g2 <- ggplot(plotpop2[c(1:6)],
             aes(x=EVTYPE_corr, y = index, fill = EVTYPE_corr))
g2 + geom_bar(stat = "identity", position = "dodge") +
        ylab("Cost Index")+
        xlab(element_blank())+
        theme_bw()+
        theme(legend.position = "none")+
        scale_fill_manual(values = prettycolors2(20))+
        theme(axis.text.x = element_text(angle = 45, hjust = 0.95)) +
        labs(title = paste0("Top 20 events in decreasing order of Cost",
                            " Index (CI).\n CI = mean of propriety damage +",
                            " mean of crop damage \n",
                            "      times total occurences (log10)\nData for ",
                            "years 1996 to 2011"))
        
```
  
I will rest my case here. Further steps should be taken to be able to consider  
this work for anything else than a simple assessment in the JH Datascience  
Course on Coursera:  
  
* In depth cleaning of the `EVTYPE_corr` variable to ensure the best merge of duplicates.  
Probing aroud with something like :  
```{r}
head(df_test_amatch[df_test_amatch$RESULT == "RIP CURRENT",])
```
... For all 48 events Would be great, manually correcting the last typos.

* Regional repartition to further affinate the specific risk potential.
* Ajusting the index, especially for health, to reflect the true impact on  
a community